{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "18sd8-wHGDR0"
   },
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "$$\n",
    "# Part 3: Transformer\n",
    "<a id=part3></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "AiCHKxjfGLRu"
   },
   "source": [
    "In this part we will implement a variation of the attention mechanism named the 'sliding window attention'. Next, we will create a transformer encoder with the sliding-window attention implementation, and we will train the encoder for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0t2UQaeKCKxa",
    "outputId": "10e4f165-d0c0-4ac9-960a-6796ff3c1750"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import unittest\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35B3L7USC5PE",
    "outputId": "5e05f218-468a-4e54-c9bb-a7357f0dfe09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "test = unittest.TestCase()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('Using device:', device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "2kRUk_GYNDd5"
   },
   "source": [
    "## Reminder: scaled dot product attention\n",
    "<a id=part3_1></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "30P7CuIwNKy-"
   },
   "source": [
    "In class, you saw that the scaled dot product attention is defined as:\n",
    "$$\n",
    "\\newcommand{\\mat}[1]{\\mathbf{#1}}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mat{B} &= \\frac{1}{\\sqrt{d}} \\mat{Q} \\mat{K}^T   \\ \\in\\set{R}^{m\\times n} \\\\\n",
    "\\mat{A} &= softmax({\\mat{B}},{\\mathrm{dim}=1}), \\in\\set{R}^{m\\times n} \\\\\n",
    "\\mat{Y} &= \\mat{A}\\mat{V} \\ \\in\\set{R}^{m\\times d_v}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where `K`,`Q` and `V` for the self attention came as projections of the same input sequnce\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\vec{q}_{i} &= \\mat{W}_{xq}\\vec{x}_{i} &\n",
    "\\vec{k}_{i} &= \\mat{W}_{xk}\\vec{x}_{i} &\n",
    "\\vec{v}_{i} &= \\mat{W}_{xv}\\vec{x}_{i} \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "If you feel the attention mechanism doesn't quite sit right, we recommend you go over lecture and tutorial notes before proceeding. \n",
    "\n",
    "We are now going to introduce a slight variation of the scaled dot product attention."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "G8CMV4VKG7YB"
   },
   "source": [
    "## Sliding window attention\n",
    "<a id=part3_2></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "YphH4DuEHP1p"
   },
   "source": [
    "The scaled dot product attention computes the dot product between **every** pair of key and query vectors. Therefore, the computation complexity is $O(n^2)$ where $n$ is the sequence length.\n",
    "\n",
    "In order to obtain a computational complexity that grows linearly with the sequnce length, the authors of 'Longformer: The Long-Document Transformer https://arxiv.org/pdf/2004.05150.pdf' proposed the 'sliding window attention' which is a variation of the scaled dot product attention. \n",
    "\n",
    "In this variation, instead of computing the dot product for every pair of key and query vectors, the dot product is only computed for keys that are in a certain 'window' around the query vector. \n",
    "\n",
    "For example, if the keys and queries are embeddings of words in the sentence \"CS is more prestigious than EE\", and the window size is 2, then for the query corresponding to the word 'is' we will only compute a dot product with the keys that are at most ${window\\_size}\\over{2}$$ = $${2}\\over{2}$$=1$ to the left and to the right. Meaning the keys that correspond to the workds 'CS', 'is' and 'more'.\n",
    "\n",
    "Formally, the intermediate calculation of the normalized dot product can be written as: \n",
    "\n",
    "$$\n",
    "\\mathrm{b}(q, k, w) \n",
    "=\n",
    "\\begin{cases}\n",
    "    q⋅k^T\\over{\\sqrt{d_k}} & \\mathrm{if} \\;d(q,k) ≤ {{w}\\over{2}} \\\\\n",
    "    -\\infty & \\mathrm{otherwise}\n",
    "\\end{cases}.\n",
    "$$\n",
    "\n",
    "Where $b(\\cdot,\\cdot,\\cdot)$ is the intermediate result function (used to construct a matrix $\\mat{B}$ on which we perform the softmax), $q$ is the query vector, $k$ is the key vector, $w$ is the sliding window size, and $d(\\cdot,\\cdot)$ is the distance function between the positions of the tokens corresponding to the key and query vectors.\n",
    "\n",
    "**Note**: The distance function $d(\\cdot,\\cdot)$ is **Not** cyclical. Meaning that that in the example above when searching for the words at distance 1 from the word 'CS', we **don't** return cyclically from the right and count the word EE.\n",
    "\n",
    "The result of this operation can be visualized like this: (green corresponds to computing the scaled dot product, and white to a no-op or $-∞$).\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*0OOTgNQFQmSa3cWYj3ZbsQ.png\" width=\"700\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "tZTy748cWIEC"
   },
   "source": [
    "**TODO**: Implement the sliding_window_attention function in hw3/transformer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CY1gz0fSebQP"
   },
   "outputs": [],
   "source": [
    "from hw3.transformer import sliding_window_attention\n",
    "\n",
    "\n",
    "## test sliding-window attention\n",
    "num_heads = 3\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "embed_dim = 3\n",
    "window_size = 2\n",
    "\n",
    "## test without extra dimension for heads\n",
    "x = torch.arange(seq_len*embed_dim).reshape(seq_len,embed_dim).repeat(batch_size,1).reshape(batch_size, seq_len, -1).float()\n",
    "\n",
    "values, attention = sliding_window_attention(x, x, x,window_size)\n",
    "\n",
    "gt_values = torch.load(os.path.join('test_tensors','values_tensor_0_heads.pt'))\n",
    "\n",
    "\n",
    "test.assertTrue(torch.all(values == gt_values), f'the tensors differ in dims [B,row,col]:{torch.stack(torch.where(values != gt_values),dim=0)}')\n",
    "\n",
    "gt_attention = torch.load(os.path.join('test_tensors','attention_tensor_0_heads.pt'))\n",
    "test.assertTrue(torch.all(attention == gt_attention), f'the tensors differ in dims [B,row,col]:{torch.stack(torch.where(attention != gt_attention),dim=0)}')\n",
    "\n",
    "\n",
    "## test with extra dimension for heads\n",
    "x = torch.arange(seq_len*embed_dim).reshape(seq_len,embed_dim).repeat(batch_size, num_heads, 1).reshape(batch_size, num_heads, seq_len, -1).float()\n",
    "\n",
    "values, attention = sliding_window_attention(x, x, x,window_size)\n",
    "\n",
    "gt_values = torch.load(os.path.join('test_tensors','values_tensor_3_heads.pt'))\n",
    "test.assertTrue(torch.all(values == gt_values), f'the tensors differ in dims [B,num_heads,row,col]:{torch.stack(torch.where(values != gt_values),dim=0)}')\n",
    "\n",
    "\n",
    "gt_attention = torch.load(os.path.join('test_tensors','attention_tensor_3_heads.pt'))\n",
    "test.assertTrue(torch.all(attention == gt_attention), f'the tensors differ in dims [B,num_heads,row,col]:{torch.stack(torch.where(attention != gt_attention),dim=0)}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "TlHiDHhgeS1_"
   },
   "source": [
    "## Multihead Sliding window attention\n",
    "<a id=part3_2></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you've seen in class, the transformer model uses a Multi-head attention module. We will use the same implementation you've seen in the tutorial, aside from the attention mechanism itslef, which will be swapped with the sliding-window attention you implemented.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Insert the call to the sliding-window attention mechanism in the forward of MultiHeadAttention in hw3/transformer.py "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "NyHo9Oy_Z7SX",
    "tags": []
   },
   "source": [
    "## Sentiment analysis\n",
    "<a id=part3_3></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6MMtJFRaZCi"
   },
   "source": [
    "We will now go on to tackling the task of sentiment analysis which is the process of analyzing text to determine if the emotional tone of the message is positive or negative (many times a neutral class is also used, but this won't be the case in the data we will be working with).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### IMBD hugging face dataset\n",
    "<a id=part3_3_1></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face is a popular open-source library and platform that provides state-of-the-art tools and resources for natural language processing (NLP) tasks. It has gained immense popularity within the NLP community due to its user-friendly interfaces, powerful pre-trained models, and a vibrant community that actively contributes to its development. \n",
    "\n",
    "Hugging Face provides a wide array of tools and utilities, which we will leverage as well. The Hugging Face Transformers library, built on top of PyTorch and TensorFlow, offers a simple yet powerful API for working with Transformer-based models (such as Distil-BERT). It enables users to easily load, fine-tune, and evaluate models, as well as generate text using these models.\n",
    "\n",
    "Furthermore, Hugging Face offers the Hugging Face Datasets library, which provides access to a vast collection of publicly available datasets for NLP. These datasets can be conveniently downloaded and used for training and evaluation purposes.\n",
    "\n",
    "You are encouraged to visit their site and see other uses: https://huggingface.co/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import pathlib\n",
    "import urllib\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the dataset using Hugging Face's `datasets` library.\n",
    "\n",
    "Feel free to look around at the full array of datasets that they offer.\n",
    "\n",
    "https://huggingface.co/docs/datasets/index\n",
    "\n",
    "We will load the full training and test sets in addition to a small toy subset of the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset('imdb', split=['train', 'test', 'train[12480:12520]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 25000\n",
      "}), Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 25000\n",
      "}), Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 40\n",
      "})]\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that it returned a list of 3 labeled datasets, the first two of size 25,000, and the third of size 40.\n",
    "We will use these as `train` and `test` datasets for training the model, and the `toy` dataset for a sanity check. \n",
    "These Datasets are wrapped in a `Dataset` class.\n",
    "\n",
    "We now wrap the dataset into a `DatasetDict` class, which contains helpful methods to use for working with the data.   \n",
    "https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#wrap it in a DatasetDict to enable methods such as map and format\n",
    "dataset = DatasetDict({'train': dataset[0], 'val': dataset[1], 'toy': dataset[2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    toy: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 40\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now access the datasets in the Dict as we would a dictionary.\n",
    "Let's print a few training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 25000\n",
      "})\n",
      "TRAINING SAMPLE 0:\n",
      "I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n",
      "Label 0: 0\n",
      "\n",
      "\n",
      "TRAINING SAMPLE 1:\n",
      "\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn't matter what one's political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn't true. I've seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don't exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we're treated to the site of Vincent Gallo's throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won't see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women's bodies.\n",
      "Label 1: 0\n",
      "\n",
      "\n",
      "TRAINING SAMPLE 2:\n",
      "If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.<br /><br />One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives (unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film).<br /><br />One might better spend one's time staring out a window at a tree growing.<br /><br />\n",
      "Label 2: 0\n",
      "\n",
      "\n",
      "TRAINING SAMPLE 3:\n",
      "This film was probably inspired by Godard's Masculin, féminin and I urge you to see that film instead.<br /><br />The film has two strong elements and those are, (1) the realistic acting (2) the impressive, undeservedly good, photo. Apart from that, what strikes me most is the endless stream of silliness. Lena Nyman has to be most annoying actress in the world. She acts so stupid and with all the nudity in this film,...it's unattractive. Comparing to Godard's film, intellectuality has been replaced with stupidity. Without going too far on this subject, I would say that follows from the difference in ideals between the French and the Swedish society.<br /><br />A movie of its time, and place. 2/10.\n",
      "Label 3: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'])\n",
    "\n",
    "for i in range(4):\n",
    "    print(f'TRAINING SAMPLE {i}:') \n",
    "    print(dataset['train'][i]['text'])\n",
    "    label = dataset['train'][i]['label']\n",
    "    print(f'Label {i}: {label}')\n",
    "    print('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should check the label distirbution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative samples in train dataset: 12500\n",
      "positive samples in train dataset: 12500\n",
      "negative samples in val dataset: 12500\n",
      "positive samples in val dataset: 12500\n",
      "negative samples in toy dataset: 20\n",
      "positive samples in toy dataset: 20\n"
     ]
    }
   ],
   "source": [
    "def label_cnt(type):\n",
    "    ds = dataset[type]\n",
    "    size = len(ds)\n",
    "    cnt= 0 \n",
    "    for smp in ds:\n",
    "        cnt += smp['label']\n",
    "    print(f'negative samples in {type} dataset: {size - cnt}')\n",
    "    print(f'positive samples in {type} dataset: {cnt}')\n",
    "    \n",
    "label_cnt('train')\n",
    "label_cnt('val')\n",
    "label_cnt('toy')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### __Import the tokenizer for the dataset__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s tokenize the texts into individual word tokens using the tokenizer implementation inherited from the pre-trained model class.  \n",
    "With Hugging Face you will always find a tokenizer associated with each model. If you are not doing research or experiments on tokenizers it’s always preferable to use the standard tokenizers.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer input max length: 512\n",
      "Tokenizer vocabulary size: 30522\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "print(\"Tokenizer input max length:\", tokenizer.model_max_length)\n",
    "print(\"Tokenizer vocabulary size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create helper functions to tokenize the text. Notice the arguments sent to the tokenizer.  \n",
    "__Padding__ is a strategy for ensuring tensors are rectangular by adding a special padding token to shorter sentences.   \n",
    "On the other hand , sometimes a sequence may be too long for a model to handle. In this case, you’ll need to __truncate__ the sequence to a shorter length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_text(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "    dataset_tokenized = dataset.map(tokenize_text, batched=True, batch_size =None)\n",
    "    return dataset_tokenized\n",
    "\n",
    "dataset_tokenized = tokenize_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we would like to work with pytorch so we can manually fine-tune\n",
    "dataset_tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# no need to parrarelize in this assignment\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### __Setting up the dataloaders and dataset__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now set up the dataloaders for efficient batching and loading of the data.  \n",
    "By now, you are familiar with the Class methods that are needed to create a working Dataloader.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.ds = dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.ds[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ds.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = IMDBDataset(dataset_tokenized['train'])\n",
    "val_dataset = IMDBDataset(dataset_tokenized['val'])\n",
    "toy_dataset = IMDBDataset(dataset_tokenized['toy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dl_train,dl_val, dl_toy = [ \n",
    "    DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=12,\n",
    "    shuffle=True, \n",
    "    num_workers=0\n",
    "),\n",
    "DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=12,\n",
    "    shuffle=True, \n",
    "    num_workers=0\n",
    "),\n",
    "DataLoader(\n",
    "    dataset=toy_dataset,\n",
    "    batch_size=4,\n",
    "    num_workers=0\n",
    ")]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "tDSwVcumaY_E",
    "tags": []
   },
   "source": [
    "### Transformer Encoder\n",
    "<a id=part3_3_2></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "qDsr_vqWa3V5"
   },
   "source": [
    "The model we will use for the task at hand, is the encoder of the transformer proposed in the seminal paper 'Attention Is All You Need'.\n",
    "\n",
    "The encoder is composed of positional encoding, and then multiple blocks which compute multi-head attention, layer normalization and a feed forward network as described in the diagram below.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"imgs/transformer_encoder.png\" alt=\"Alternative text\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We provided you with implemetations for the positional encoding and the position-wise feed forward MLP in hw3/transformer.py. \n",
    "\n",
    "Feel free to read through the implementations to make sure you understand what they do."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: To begin with, complete the transformer EncoderLayer in hw3/transformer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from hw3.transformer import EncoderLayer\n",
    "# set torch seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "layer = EncoderLayer(embed_dim=16, hidden_dim=16, num_heads=4, window_size=4, dropout=0.1)\n",
    "\n",
    "# load x and y\n",
    "x = torch.load(os.path.join('test_tensors','encoder_layer_input.pt'))\n",
    "y = torch.load(os.path.join('test_tensors','encoder_layer_output.pt'))\n",
    "padding_mask = torch.ones(2, 10)\n",
    "padding_mask[:, 5:] = 0\n",
    "\n",
    "# forward pass\n",
    "out = layer(x, padding_mask)\n",
    "test.assertTrue(torch.allclose(out, y, atol=1e-6), 'output of encoder layer is incorrect')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to classify a sentence using the encoder, we need to somehow summarize the output of the last encoder layer (which will include an output for each token in the tokenized input sentence). \n",
    "\n",
    "There are several options for doing this. We will use the output of the special token [CLS] appended to the beginning of each sentence by the bert tokenizer we are using.\n",
    "\n",
    "Let's see an example of the first tokens in a sentence after tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'i', 'rented', 'i', 'am', 'curious', '-', 'yellow', 'from', 'my']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(dataset_tokenized['train'][0]['input_ids'])[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**TODO**: Now it's time to put it all together. Complete the implementaion of 'Encoder' in hw3/transformer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Index tensor must have the same number of dimensions as input tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m padding_mask[:, \u001b[38;5;241m50\u001b[39m:] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m test\u001b[38;5;241m.\u001b[39massertTrue(torch\u001b[38;5;241m.\u001b[39mallclose(out, y, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput of encoder layer is incorrect\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\cs236781-hw3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\OneDrive - Technion\\Deep Learning\\hw3w26\\hw3\\transformer.py:278\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, sentence, padding_mask)\u001b[0m\n\u001b[0;32m    276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_layers:\n\u001b[1;32m--> 278\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m classifcation_tokens \u001b[38;5;241m=\u001b[39m x[:, \u001b[38;5;241m0\u001b[39m, :]\n\u001b[0;32m    280\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassification_mlp(classifcation_tokens)\n",
      "File \u001b[1;32m~\\.conda\\envs\\cs236781-hw3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\OneDrive - Technion\\Deep Learning\\hw3w26\\hw3\\transformer.py:225\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[1;34m(self, x, padding_mask)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;124;03m:param x: the input to the layer of shape [Batch, SeqLen, Dims]\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;124;03m:param padding_mask: the padding mask of shape [Batch, SeqLen]\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;124;03m:return: the output of the layer of shape [Batch, SeqLen, Dims]\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;66;03m# ====== YOUR CODE: ======\u001b[39;00m\n\u001b[1;32m--> 225\u001b[0m attn_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m attn_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attn_x)\n\u001b[0;32m    227\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(attn_x \u001b[38;5;241m+\u001b[39m x)\n",
      "File \u001b[1;32m~\\.conda\\envs\\cs236781-hw3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\OneDrive - Technion\\Deep Learning\\hw3w26\\hw3\\transformer.py:148\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, x, padding_mask, return_attention)\u001b[0m\n\u001b[0;32m    143\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m qkv\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m3\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#[Batch, Head, SeqLen, Dims]\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# Determine value outputs\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# call the sliding window attention function you implemented\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# ====== YOUR CODE: ======\u001b[39;00m\n\u001b[1;32m--> 148\u001b[0m values, attention \u001b[38;5;241m=\u001b[39m \u001b[43msliding_window_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# ========================\u001b[39;00m\n\u001b[0;32m    151\u001b[0m values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m) \u001b[38;5;66;03m# [Batch, SeqLen, Head, Dims]\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive - Technion\\Deep Learning\\hw3w26\\hw3\\transformer.py:88\u001b[0m, in \u001b[0;36msliding_window_attention\u001b[1;34m(q, k, v, window_size, padding_mask)\u001b[0m\n\u001b[0;32m     85\u001b[0m idx_clamped \u001b[38;5;241m=\u001b[39m idx\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m0\u001b[39m, seq_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [L, 2w+1]\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# gather key-validity for each neighbor (0 if PAD)\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m neigh_ok \u001b[38;5;241m=\u001b[39m \u001b[43mpadding_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx_clamped\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [B, L, 2w+1]\u001b[39;00m\n\u001b[0;32m     91\u001b[0m neigh_ok \u001b[38;5;241m=\u001b[39m neigh_ok\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [B, 1, L, 2w+1]\u001b[39;00m\n\u001b[0;32m     92\u001b[0m scores_local \u001b[38;5;241m=\u001b[39m scores_local\u001b[38;5;241m.\u001b[39mmasked_fill(neigh_ok \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, neg_number)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Index tensor must have the same number of dimensions as input tensor"
     ]
    }
   ],
   "source": [
    "from hw3.transformer import Encoder\n",
    "\n",
    "# set torch seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "encoder = Encoder(vocab_size=100, embed_dim=16, num_heads=4, num_layers=3, \n",
    "                  hidden_dim=16, max_seq_length=64, window_size=4, dropout=0.1)\n",
    "\n",
    "\n",
    "# load x and y\n",
    "x = torch.load(os.path.join('test_tensors','encoder_input.pt'))\n",
    "y = torch.load(os.path.join('test_tensors','encoder_output.pt'))\n",
    "#x = torch.randint(0, 100, (2, 64)).long()\n",
    "\n",
    "padding_mask = torch.ones(2, 64)\n",
    "padding_mask[:, 50:] = 0\n",
    "\n",
    "# forward pass\n",
    "out = encoder(x, padding_mask)\n",
    "test.assertTrue(torch.allclose(out, y, atol=1e-6), 'output of encoder layer is incorrect')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the encoder\n",
    "<a id=part3_3_3></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now proceed to train the model. \n",
    "\n",
    "**TODO**: Complete the implementation of TransformerEncoderTrainer in hw3/training.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training on a toy dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, we will train on a small toy dataset of 40 samples. This will serve as a sanity check to make sure nothing is buggy.\n",
    "\n",
    "**TODO**: choose the hyperparameters in hw3.answers part3_transformer_encoder_hyperparams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "VWHBI8gm2qo-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embed_dim': 128, 'num_heads': 8, 'num_layers': 2, 'hidden_dim': 256, 'window_size': 128, 'droupout': 0.0, 'lr': 0.001, 'dropout': 0.2}\n"
     ]
    }
   ],
   "source": [
    "from hw3.answers import part3_transformer_encoder_hyperparams\n",
    "\n",
    "params = part3_transformer_encoder_hyperparams()\n",
    "print(params)\n",
    "embed_dim = params['embed_dim'] \n",
    "num_heads = params['num_heads']\n",
    "num_layers = params['num_layers']\n",
    "hidden_dim = params['hidden_dim']\n",
    "window_size = params['window_size']\n",
    "dropout = params['droupout']\n",
    "lr = params['lr']\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "max_seq_length = tokenizer.model_max_length\n",
    "\n",
    "max_batches_per_epoch = None\n",
    "N_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_model = Encoder(vocab_size, embed_dim, num_heads, num_layers, hidden_dim, max_seq_length, window_size, dropout=dropout).to(device)\n",
    "toy_optimizer = optim.Adam(toy_model.parameters(), lr=lr)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- EPOCH 1/20 ---\n",
      "train_batch (Avg. Loss 1.540, Accuracy 50.0): 100%|████████████████████████████████████| 10/10 [00:11<00:00,  1.16s/it]\n",
      "test_batch (Avg. Loss 0.740, Accuracy 50.0): 100%|█████████████████████████████████████| 10/10 [00:05<00:00,  1.98it/s]\n",
      "*** Saved checkpoint toy_transfomer_encoder.pt at epoch 1\n",
      "--- EPOCH 2/20 ---\n",
      "train_batch (Avg. Loss 0.966, Accuracy 30.0): 100%|████████████████████████████████████| 10/10 [00:11<00:00,  1.13s/it]\n",
      "test_batch (Avg. Loss 0.697, Accuracy 50.0): 100%|█████████████████████████████████████| 10/10 [00:04<00:00,  2.07it/s]\n",
      "*** Saved checkpoint toy_transfomer_encoder.pt at epoch 2\n",
      "--- EPOCH 3/20 ---\n",
      "train_batch (Avg. Loss 0.804, Accuracy 10.0): 100%|████████████████████████████████████| 10/10 [00:10<00:00,  1.04s/it]\n",
      "test_batch (Avg. Loss 0.692, Accuracy 50.0): 100%|█████████████████████████████████████| 10/10 [00:04<00:00,  2.20it/s]\n",
      "*** Saved checkpoint toy_transfomer_encoder.pt at epoch 3\n",
      "--- EPOCH 4/20 ---\n",
      "train_batch (Avg. Loss 0.744, Accuracy 20.0): 100%|████████████████████████████████████| 10/10 [00:10<00:00,  1.05s/it]\n",
      "test_batch (Avg. Loss 0.693, Accuracy 50.0): 100%|█████████████████████████████████████| 10/10 [00:04<00:00,  2.10it/s]\n",
      "Early stopping at epoch 4\n",
      "*** Saved checkpoint toy_transfomer_encoder.pt at epoch 4\n",
      "--- EPOCH 5/20 ---\n",
      "train_batch (Avg. Loss 0.745, Accuracy 2.5): 100%|█████████████████████████████████████| 10/10 [00:10<00:00,  1.09s/it]\n",
      "test_batch (Avg. Loss 0.693, Accuracy 50.0): 100%|█████████████████████████████████████| 10/10 [00:04<00:00,  2.07it/s]\n",
      "Early stopping at epoch 5\n",
      "*** Saved checkpoint toy_transfomer_encoder.pt at epoch 5\n",
      "--- EPOCH 6/20 ---\n",
      "train_batch (Avg. Loss 0.751, Accuracy 0.0): 100%|█████████████████████████████████████| 10/10 [00:10<00:00,  1.08s/it]\n",
      "test_batch (Avg. Loss 0.690, Accuracy 50.0): 100%|█████████████████████████████████████| 10/10 [00:04<00:00,  2.20it/s]\n",
      "*** Saved checkpoint toy_transfomer_encoder.pt at epoch 10\n",
      "--- EPOCH 11/20 ---\n",
      "train_batch (Avg. Loss 0.735, Accuracy 0.0): 100%|█████████████████████████████████████| 10/10 [00:10<00:00,  1.05s/it]\n",
      "test_batch (Avg. Loss 0.688, Accuracy 50.0): 100%|█████████████████████████████████████| 10/10 [00:04<00:00,  2.23it/s]\n",
      "*** Saved checkpoint toy_transfomer_encoder.pt at epoch 11\n",
      "--- EPOCH 12/20 ---\n",
      "train_batch (Avg. Loss 0.731, Accuracy 2.5): 100%|█████████████████████████████████████| 10/10 [00:10<00:00,  1.07s/it]\n",
      "test_batch (Avg. Loss 0.684, Accuracy 50.0): 100%|█████████████████████████████████████| 10/10 [00:04<00:00,  2.24it/s]\n",
      "*** Saved checkpoint toy_transfomer_encoder.pt at epoch 12\n",
      "--- EPOCH 13/20 ---\n",
      "train_batch (Avg. Loss 0.725, Accuracy 10.0): 100%|████████████████████████████████████| 10/10 [00:10<00:00,  1.10s/it]\n",
      "test_batch (Avg. Loss 0.674, Accuracy 50.0): 100%|█████████████████████████████████████| 10/10 [00:04<00:00,  2.09it/s]\n",
      "*** Saved checkpoint toy_transfomer_encoder.pt at epoch 13\n",
      "--- EPOCH 14/20 ---\n",
      "train_batch (Avg. Loss 0.703, Accuracy 25.0): 100%|████████████████████████████████████| 10/10 [00:10<00:00,  1.08s/it]\n",
      "test_batch (Avg. Loss 0.606, Accuracy 90.0): 100%|█████████████████████████████████████| 10/10 [00:04<00:00,  2.12it/s]\n",
      "*** Saved checkpoint toy_transfomer_encoder.pt at epoch 14\n",
      "--- EPOCH 15/20 ---\n",
      "train_batch (Avg. Loss 0.440, Accuracy 92.5): 100%|████████████████████████████████████| 10/10 [00:10<00:00,  1.07s/it]\n",
      "test_batch (Avg. Loss 0.228, Accuracy 100.0): 100%|████████████████████████████████████| 10/10 [00:04<00:00,  2.17it/s]\n",
      "*** Saved checkpoint toy_transfomer_encoder.pt at epoch 15\n",
      "--- EPOCH 16/20 ---\n",
      "train_batch (Avg. Loss 0.094, Accuracy 100.0): 100%|███████████████████████████████████| 10/10 [00:10<00:00,  1.04s/it]\n",
      "test_batch (Avg. Loss 0.019, Accuracy 100.0): 100%|████████████████████████████████████| 10/10 [00:04<00:00,  2.16it/s]\n",
      "*** Saved checkpoint toy_transfomer_encoder.pt at epoch 16\n",
      "--- EPOCH 17/20 ---\n",
      "train_batch (Avg. Loss 0.013, Accuracy 100.0): 100%|███████████████████████████████████| 10/10 [00:10<00:00,  1.04s/it]\n",
      "test_batch (Avg. Loss 0.007, Accuracy 100.0): 100%|████████████████████████████████████| 10/10 [00:04<00:00,  2.18it/s]\n",
      "*** Saved checkpoint toy_transfomer_encoder.pt at epoch 17\n",
      "--- EPOCH 18/20 ---\n",
      "train_batch (Avg. Loss 0.006, Accuracy 100.0): 100%|███████████████████████████████████| 10/10 [00:10<00:00,  1.03s/it]\n",
      "test_batch (Avg. Loss 0.005, Accuracy 100.0): 100%|████████████████████████████████████| 10/10 [00:04<00:00,  2.14it/s]\n",
      "*** Saved checkpoint toy_transfomer_encoder.pt at epoch 18\n",
      "--- EPOCH 19/20 ---\n",
      "train_batch (Avg. Loss 0.004, Accuracy 100.0): 100%|███████████████████████████████████| 10/10 [00:10<00:00,  1.00s/it]\n",
      "test_batch (Avg. Loss 0.004, Accuracy 100.0): 100%|████████████████████████████████████| 10/10 [00:04<00:00,  2.18it/s]\n",
      "*** Saved checkpoint toy_transfomer_encoder.pt at epoch 19\n",
      "--- EPOCH 20/20 ---\n",
      "train_batch (Avg. Loss 0.003, Accuracy 100.0): 100%|███████████████████████████████████| 10/10 [00:10<00:00,  1.03s/it]\n",
      "test_batch (Avg. Loss 0.003, Accuracy 100.0): 100%|████████████████████████████████████| 10/10 [00:04<00:00,  2.21it/s]\n",
      "*** Saved checkpoint toy_transfomer_encoder.pt at epoch 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # fit your model\n",
    "# import pickle\n",
    "if not os.path.exists('toy_transfomer_encoder.pt'):\n",
    "    # overfit\n",
    "    from hw3.training import TransformerEncoderTrainer\n",
    "    toy_trainer = TransformerEncoderTrainer(toy_model, criterion, toy_optimizer, device=device)\n",
    "    # set max batches per epoch\n",
    "    _ = toy_trainer.fit(dl_toy, dl_toy, N_EPOCHS, checkpoints='toy_transfomer_encoder', max_batches=max_batches_per_epoch)\n",
    "\n",
    "    \n",
    "\n",
    "toy_saved_state = torch.load('toy_transfomer_encoder.pt')\n",
    "toy_best_acc = toy_saved_state['best_acc']\n",
    "toy_model.load_state_dict(toy_saved_state['model_state']) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test.assertTrue(toy_best_acc >= 95)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training on all data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You are now ready to train your sentiment analysis classifier!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_batches_per_epoch = 500\n",
    "N_EPOCHS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Encoder(vocab_size, embed_dim, num_heads, num_layers, hidden_dim, max_seq_length, window_size, dropout).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- EPOCH 1/4 ---\n",
      "train_batch (Avg. Loss 0.671, Accuracy 57.3): 100%|██████████████████████████████████| 500/500 [22:52<00:00,  2.75s/it]\n",
      "test_batch (0.706):  89%|█████████████████████████████████████████████████████▏      | 443/500 [09:55<01:16,  1.34s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m TransformerEncoderTrainer(model, criterion, optimizer, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# set max batches per epoch\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrained_transfomer_encoder\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_batches_per_epoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m saved_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrained_transfomer_encoder.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m best_acc \u001b[38;5;241m=\u001b[39m saved_state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_acc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\OneDrive - Technion\\Deep Learning\\hw3w26\\hw3\\training.py:100\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, dl_train, dl_test, num_epochs, checkpoints, early_stopping, print_every, post_epoch_fn, max_batches, **kw)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# TODO:\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m#  Train & evaluate for one epoch\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m#  - Use the train/test_epoch methods.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m#    simple regularization technique that is highly recommended.\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# ====== YOUR CODE: ======\u001b[39;00m\n\u001b[0;32m     99\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_epoch(dl_train, max_batches\u001b[38;5;241m=\u001b[39mmax_batches)\n\u001b[1;32m--> 100\u001b[0m test_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_batches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mtensor(train_result\u001b[38;5;241m.\u001b[39mlosses)\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m    102\u001b[0m train_acc\u001b[38;5;241m.\u001b[39mappend(train_result\u001b[38;5;241m.\u001b[39maccuracy)\n",
      "File \u001b[1;32m~\\OneDrive - Technion\\Deep Learning\\hw3w26\\hw3\\training.py:156\u001b[0m, in \u001b[0;36mTrainer.test_epoch\u001b[1;34m(self, dl_test, **kw)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03mEvaluate model once over a test set (single epoch).\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03m:param dl_test: DataLoader for the test set.\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;124;03m:param kw: Keyword args supported by _foreach_batch.\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;124;03m:return: An EpochResult for the epoch.\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# set evaluation (test) mode\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive - Technion\\Deep Learning\\hw3w26\\hw3\\training.py:220\u001b[0m, in \u001b[0;36mTrainer._foreach_batch\u001b[1;34m(dl, forward_fn, verbose, max_batches)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[0;32m    219\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dl_iter)\n\u001b[1;32m--> 220\u001b[0m     batch_res \u001b[38;5;241m=\u001b[39m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpbar_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_res\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    223\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[1;32m~\\OneDrive - Technion\\Deep Learning\\hw3w26\\hw3\\training.py:384\u001b[0m, in \u001b[0;36mTransformerEncoderTrainer.test_batch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    379\u001b[0m num_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# TODO:\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m#  fill out the testing loop.\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# ====== YOUR CODE: ======\u001b[39;00m\n\u001b[1;32m--> 384\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    385\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(x\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), label)\n\u001b[0;32m    386\u001b[0m num_correct \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mround(torch\u001b[38;5;241m.\u001b[39msigmoid(x))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m label)\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[1;32m~\\.conda\\envs\\cs236781-hw3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\OneDrive - Technion\\Deep Learning\\hw3w26\\hw3\\transformer.py:263\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, sentence, padding_mask)\u001b[0m\n\u001b[0;32m    261\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_layers:\n\u001b[1;32m--> 263\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    264\u001b[0m classifcation_tokens \u001b[38;5;241m=\u001b[39m x[:, \u001b[38;5;241m0\u001b[39m, :]\n\u001b[0;32m    265\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassification_mlp(classifcation_tokens)\n",
      "File \u001b[1;32m~\\.conda\\envs\\cs236781-hw3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\OneDrive - Technion\\Deep Learning\\hw3w26\\hw3\\transformer.py:210\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[1;34m(self, x, padding_mask)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m:param x: the input to the layer of shape [Batch, SeqLen, Dims]\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m:param padding_mask: the padding mask of shape [Batch, SeqLen]\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m:return: the output of the layer of shape [Batch, SeqLen, Dims]\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;66;03m# ====== YOUR CODE: ======\u001b[39;00m\n\u001b[1;32m--> 210\u001b[0m attn_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m attn_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attn_x)\n\u001b[0;32m    212\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(attn_x \u001b[38;5;241m+\u001b[39m x)\n",
      "File \u001b[1;32m~\\.conda\\envs\\cs236781-hw3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\OneDrive - Technion\\Deep Learning\\hw3w26\\hw3\\transformer.py:133\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, x, padding_mask, return_attention)\u001b[0m\n\u001b[0;32m    128\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m qkv\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m3\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#[Batch, Head, SeqLen, Dims]\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# Determine value outputs\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# call the sliding window attention function you implemented\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# ====== YOUR CODE: ======\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m values, attention \u001b[38;5;241m=\u001b[39m \u001b[43msliding_window_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# ========================\u001b[39;00m\n\u001b[0;32m    136\u001b[0m values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m) \u001b[38;5;66;03m# [Batch, SeqLen, Head, Dims]\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive - Technion\\Deep Learning\\hw3w26\\hw3\\transformer.py:47\u001b[0m, in \u001b[0;36msliding_window_attention\u001b[1;34m(q, k, v, window_size, padding_mask)\u001b[0m\n\u001b[0;32m     44\u001b[0m v_win \u001b[38;5;241m=\u001b[39m v_pad\u001b[38;5;241m.\u001b[39munfold(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m w \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# ---- local scores: [B,H,L,2w+1] ----\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m scores_local \u001b[38;5;241m=\u001b[39m (\u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk_win\u001b[49m)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# ---- mask out-of-range neighbors at boundaries ----\u001b[39;00m\n\u001b[0;32m     50\u001b[0m offsets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m-\u001b[39mw, w \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, device\u001b[38;5;241m=\u001b[39mq\u001b[38;5;241m.\u001b[39mdevice)                 \u001b[38;5;66;03m# [2w+1]\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fit your model\n",
    "import pickle\n",
    "if not os.path.exists('trained_transfomer_encoder.pt'):\n",
    "    from hw3.training import TransformerEncoderTrainer\n",
    "    trainer = TransformerEncoderTrainer(model, criterion, optimizer, device=device)\n",
    "    # set max batches per epoch\n",
    "    _ = trainer.fit(dl_train, dl_val, N_EPOCHS, checkpoints='trained_transfomer_encoder', max_batches=max_batches_per_epoch)\n",
    "    \n",
    "\n",
    "saved_state = torch.load('trained_transfomer_encoder.pt')\n",
    "best_acc = saved_state['best_acc']\n",
    "model.load_state_dict(saved_state['model_state']) \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test.assertTrue(best_acc >= 65)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the follwing cells to see an example of the model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rand_index = torch.randint(len(dataset_tokenized['val']), (1,))\n",
    "rand_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = dataset['val'][rand_index]\n",
    "sample['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_sample = dataset_tokenized['val'][rand_index]\n",
    "tokenized_sample\n",
    "input_ids = tokenized_sample['input_ids'].to(device)\n",
    "label = tokenized_sample['label'].to(device)\n",
    "attention_mask = tokenized_sample['attention_mask'].to(float).to(device)\n",
    "\n",
    "print('label', label.shape)\n",
    "print('attention_mask', attention_mask.shape)\n",
    "prediction = model.predict(input_ids, attention_mask).squeeze(0)\n",
    "\n",
    "print('label: {}, prediction: {}'.format(label, prediction))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next part you wil see how to fine-tune a pretrained model for the same task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from cs236781.answers import display_answer\n",
    "import hw3.answers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill your answers in hw3.answers.part3_q1 and hw3.answers.part3_q2 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Question 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain why stacking encoder layers that use the sliding-window attention results in a broader context in the final layer.\n",
    "Hint: Think what happens when stacking CNN layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_answer(hw3.answers.part3_q1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Propose a variation of the attention pattern such that the computational complexity stays similar to that of the sliding-window attention O(nw), but the attention is computed on a more global context. Analyze the new time complexity, and how global information would be shared. Would it take many layers? Are there limitations to this information sharing?\n",
    "Note: There is no single correct answer to this, feel free to read the paper that proposed the sliding-window. Any solution that makes sense will be considered correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_answer(hw3.answers.part3_q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
